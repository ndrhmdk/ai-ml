{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neurons for Visions**\n",
    "## **Designing the Neural Network**\n",
    "First, we'll look at the design of the neural network in *Figure 2-5*\n",
    "![Extending our pattern for a more complex example](fig2.5.png)\n",
    "\n",
    "```python\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "```\n",
    "\n",
    "The first, `Flatten` isn't a layer of neurons, but an input layer specification. Our inputs are $28\\times 28$ images, but we want them to be treated as a series of numeric values, like a gray boxes at the top of the *Figure 2-5*. `Flatten` takes that \"square\" value (a 2D array) and turns it into a line (a 1D array).\n",
    "\n",
    "The next one, `Dense`, is a layer of neurons, and we're specifying that we want 128 of them. This is the middle layer shown in *Figure 2-5*. You'll often hear such layers described as $\\textbf{\\textit{hidden layers}}$. Layers that are between the inputs and the outputs aren't seen by a caller, so the term \"hidden\" is used to describe them. We are asking for $128$ neurons to have their internal parameters randomly initialized. More neurons means it will run more slowly, as it has to learn more parameters. More neurons could also lead to a network that is greate at recognizing the training data, but not so good at recognizing data that it hasn't previously seen (this is known as $\\textit{overfitting}$). On the other hand, fewer neurons means that the model might not have sufficient parameters to learn.\n",
    "\n",
    "It takes some experimentation over time to pick the right values. This process is typically called $\\textbf{\\textit{hyperparameter\n",
    "tuning}}$. In machine learning, a hyperparameter is a value that is used to control the training, as opposed to the internal values of the neurons that get trained/learned, which are referred to as parameters.\n",
    "\n",
    "There's also an $\\textit{activation function}$ specified in that layer. The activation function is code that will execute on each neuron in the layer. TensorFlow supports a number of them, but a very common one in middle layers is `relu`, which stands for $\\textit{rectified linear unit}$. It's a simple function that just returns a value if it's greater than $0$. In this case, we don't want negative values being passed to the next layer to potentially impact the summing function, so instead of writing a lot of `if-then` code, we can simply activate the layer with `relu`.\n",
    "\n",
    "Finally, there's another `Dense` layer, which is the output layer. This has $10$ neurons, because we have $10$ classes. Each of these neurons will end up with a probability that the input pixels match that class, so our job is to determine which one has the highest values. We could loop through them to pick that value, but the `softmax` activation function does that for us.\n",
    "\n",
    "Now when we train our neural network, the goal is that we can feed in a $28 \\times 28$-pixel array and the neurons in the middle layer will have weights and bias ($m$ and $c$ values) that when combined will match those pixels to one of the $10$ output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7802 - loss: 0.6356\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8601 - loss: 0.3897\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 943us/step - accuracy: 0.8742 - loss: 0.3454\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step - accuracy: 0.8833 - loss: 0.3178\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 939us/step - accuracy: 0.8911 - loss: 0.2934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25560e80fe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First is a handy shortcut for accessing the data:\n",
    "```python\n",
    "    data = tf.keras.datasets.fashion_mnist\n",
    "```\n",
    "`keras` has a number of built-in datasets that you can access with a single line of code like this. In this case you don't have to handle downloading the $70,000$ images-spliting them into training and test sets, and so on-all it takes is one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call its `load_data` method to return our training and test sets like this:\n",
    "```python\n",
    "    (training_images, training_labels), \n",
    "    (test_images, test_labels) = data.load_data()\n",
    "```\n",
    "Fashion MNIST is designed to have $60,000$ training images and $10,000$ test images. So the return from `data.load_data` will give you an array of $60,000 $ $28 \\times 28$-pixel arrays called `training_images`, and an array of $60,000$ values (0-9) called `training_labels`. Similarly, the `test_images` will contain $10,000$ 28 $\\times$ 28-pixel arrays, and the `test_labels` array will contain $10,000$ values between 0 and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    training_images  = training_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "```\n",
    "Python allows you to do an operation across the entire array with this notation. Recall that all of the pixels in our images are grayscale, with values between 0 and 255. Dividing by 255 thus ensures that every pixel is represented by a number between 0 and 1 instead. This process is called $\\textbf{\\textit{normalizing}}$ the image.\n",
    "\n",
    "Normalization will improve performance. Often your network will not learn and will have massive errors when dealing with non normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the neural network that makes up our model, as discussed ealier:\n",
    "```python\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "```\n",
    "When we compile our model we specify the loss function and the optimizer as before:\n",
    "```python\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "```\n",
    "The loss function in this case is called $\\textit{sparse categorical cross entropy}$, and it's one of the arsenal of loss functions that are built into TensorFlow. Again, choosing which loss function to use is an art in itself, and over time you'll learn which ones are best to use in which scenarios. Here we're picking a *category*. Our item of clothing will belong to $1$ of $10$ categories of clothing, and thus using a *categorical* loss function is the way to go. Sparse categorical cross entropy is a good choice\n",
    "\n",
    "The same applies to choosing an optimizer. The `adam` optimizer is an evolution of the stochastic gradient descent (`sgd`) optimizers that has been shown to be faster and more efficient. As we're handling $60,000$ training images, any performance improvement we can get will be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train the network by fitting the training images to the training labels over five epochs:\n",
    "```python\n",
    "    model.fit(training_images, training_labels, epochs=5)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do something new-evaluate the model, using a single line of code. We have a set of $10,000$ images and lebels for testing, and we can pass them to the trained model to have it predict what it thinks each image is, compare that to its actual label, and sum up the result:\n",
    "```python\n",
    "    model.evaluate(test_images, test_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the Neural Network**\n",
    "```python\n",
    "    58016/60000 [=====>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8907\n",
    "    59552/60000 [=====>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8906\n",
    "    60000/60000 [] - 2s 34us/sample - loss: 0.2940 - accuracy: 0.8906\n",
    "```\n",
    "Note that now it's now reporting accuracy. So in this case, using the training data, our model ended up with an accuracy of about $89\\%$ after only five epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the test data? The result of `model.evaluate` on our test data will look something like this:\n",
    "```python\n",
    "    10000/1 [====] - 0s 30us/sample - loss: 0.2521 - accuracy: 0.8736\n",
    "```\n",
    "In this case the accuracy of the model was $87.36\\%$, which isn't bad considering we only trained it for five epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably wondering why the accuracy is *lower* for the test data than it is for the training data. This is very commonly seen but it makes sense: the neural network only really knows how to match the inputs it has been trained on with the ouputs for those values. Our hope is that, given enough data, it will be able to generalize from the examples it has seen, \"learning\" what a shoe or a dress looks like. But there will always be examples of items that is hasn't seen that are sufficiently different from what it has to confuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring the Model Output**\n",
    "Now that the model has been trained, and we have a good gage of its accuracy using the test set, let's explore it a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step\n",
      "[8.61319450e-07 6.27786623e-09 9.87816406e-07 1.45292930e-07\n",
      " 8.74490797e-06 2.54593417e-02 4.99701719e-06 5.83301485e-02\n",
      " 1.14903996e-04 9.16079819e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification gives us back an array of values. These are the values of the $10$ output neurons. The label is the actual label for the item of clothing, in this case $9$. You'll see that some of the values are very small, and the last one is the largest by far. These are the probabilities that the image matches the label at that particular index. So, what the neural network is reporting is that there's a $91.4\\%$ chance that the item of clothing at index $0$ is label $9$. We know that it's label $9$, so it got it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training for Longer-Discovering Overfitting**\n",
    "In this case, we trained for only five epochs. That is, we went through the entire training loop of having the neurons randomly initialized, checked against their labels, having that performance measured by the loss function, and then updated by the optimizer five times. And the result we got were pretty good: $89\\%$ accuracy on the training set and $87\\%$ on the test set.\n",
    "\n",
    "Try to update it to train for $50$ epochs instead of $5$. In this case, we got these accuracy figures on the training set:\n",
    "```python\n",
    "    58112/60000 [==>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9627\n",
    "    59520/60000 [==>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9627\n",
    "    60000/60000 [====] - 2s 35us/sample - loss: 0.0986 - accuracy: 0.9627\n",
    "```\n",
    "\n",
    "This is particularly exciting because we're doing much better: $96.27\\%$ accuracy. For the test set we reach $88.6\\%$\n",
    "```python\n",
    "    [====] - 0s 30us/sample - loss: 0.3870 - accuracy: 0.8860\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got a big improvement on the training set, and a smaller one on the test set. This might suggest that training our network for much longer would lead to much better results—but that’s not always the case. The network is doing much better with the training data, but it’s not necessarily a better model. In fact, the divergence in the accuracy numbers shows that it has become overspecialized to the training data, a process often called $\\textbf{\\textit{overfitting}}$. As you build more neural networks this is something to watch out for, and as you go through this book you’ll learn a number of techniques to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stopping Training**\n",
    "In each of the cases so far, we've hardcoded the number of epochs we're training for. While that works, we might want to train until we reach the desired accuracy instead of constantly trying different numbers of epochs and training and retraining until we get to our desired value. So, for example, if we want to train until the model is at $95\\%$ accuracy on the training set, without knowing how many epochs that will take, how could we do that?\n",
    "\n",
    "The easiest approach is to use a $\\textbf{\\textit{callback}}$ on the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.0070 - loss: 5.1278\n",
      "Epoch 2/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0076 - loss: 4.8765\n",
      "Epoch 3/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0090 - loss: 4.8466\n",
      "Epoch 4/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0123 - loss: 4.8284\n",
      "Epoch 5/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0133 - loss: 4.8071\n",
      "Epoch 6/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0144 - loss: 4.7896\n",
      "Epoch 7/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0146 - loss: 4.7797\n",
      "Epoch 8/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0130 - loss: 4.7661\n",
      "Epoch 9/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0141 - loss: 4.7557\n",
      "Epoch 10/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0155 - loss: 4.7466\n",
      "Epoch 11/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0130 - loss: 4.7443\n",
      "Epoch 12/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0135 - loss: 4.7384\n",
      "Epoch 13/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0125 - loss: 4.7314\n",
      "Epoch 14/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0131 - loss: 4.7240\n",
      "Epoch 15/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0128 - loss: 4.7239\n",
      "Epoch 16/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0130 - loss: 4.7117\n",
      "Epoch 17/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0140 - loss: 4.7098\n",
      "Epoch 18/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0121 - loss: 4.7121\n",
      "Epoch 19/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0131 - loss: 4.7000\n",
      "Epoch 20/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0131 - loss: 4.6967\n",
      "Epoch 21/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0129 - loss: 4.6987\n",
      "Epoch 22/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0125 - loss: 4.6957\n",
      "Epoch 23/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0120 - loss: 4.6865\n",
      "Epoch 24/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0148 - loss: 4.6878\n",
      "Epoch 25/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0123 - loss: 4.6839\n",
      "Epoch 26/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0126 - loss: 4.6822\n",
      "Epoch 27/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0127 - loss: 4.6809\n",
      "Epoch 28/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0117 - loss: 4.6772\n",
      "Epoch 29/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0120 - loss: 4.6724\n",
      "Epoch 30/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0115 - loss: 4.6666\n",
      "Epoch 31/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0129 - loss: 4.6690\n",
      "Epoch 32/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0137 - loss: 4.6649\n",
      "Epoch 33/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0108 - loss: 4.6708\n",
      "Epoch 34/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0116 - loss: 4.6679\n",
      "Epoch 35/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0109 - loss: 4.6622\n",
      "Epoch 36/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0110 - loss: 4.6715\n",
      "Epoch 37/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0118 - loss: 4.6594\n",
      "Epoch 38/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0116 - loss: 4.6537\n",
      "Epoch 39/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0110 - loss: 4.6590\n",
      "Epoch 40/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0110 - loss: 4.6600\n",
      "Epoch 41/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0113 - loss: 4.6578\n",
      "Epoch 42/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0109 - loss: 4.6483\n",
      "Epoch 43/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0112 - loss: 4.6524\n",
      "Epoch 44/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0105 - loss: 4.6596\n",
      "Epoch 45/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0123 - loss: 4.6487\n",
      "Epoch 46/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0111 - loss: 4.6468\n",
      "Epoch 47/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0107 - loss: 4.6448\n",
      "Epoch 48/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0114 - loss: 4.6493\n",
      "Epoch 49/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0111 - loss: 4.6441\n",
      "Epoch 50/50\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0106 - loss: 4.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24d299878c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.95):\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential({\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "})\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a new class called `myCallback`. This takes a `tf.keras.callbacks.Callback` as a parameter. In it, we define the `on_epoch_end` function, which will give us details about the logs for this epoch. In these logs is an accuracy value, so all we have to do is see if it is greater than $.95$; if it is, we can stop training by saying `self.model.stop_training = True`\n",
    "\n",
    "Once we've specified this, we create a `callbacks` object to be an instance of the `myCallback` function.\n",
    "\n",
    "Now check out the `model.fit` statement. You'll see that I've updated it to train for 50 epochs, and then added a `callbacks` parameter. To this, I pass the `callbacks` object.\n",
    "\n",
    "When training, at the end of every epoch, the callback funcion will be called. So at the end of each epoch you'll check, and after about 34 epochs you'll ee that your training will end, because the training has hit $95\\%$ accuracy:\n",
    "```python\n",
    "    56896/60000 [====>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9500\n",
    "    58144/60000 [====>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9502\n",
    "    59424/60000 [====>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9502\n",
    "    Reached 95% accuracy so cancelling training!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
