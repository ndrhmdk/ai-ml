{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T11:04:10.164690Z",
     "start_time": "2024-11-18T11:04:03.900901Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neurons for Visions**\n",
    "## **Designing the Neural Network**\n",
    "First, we'll look at the design of the neural network in *Figure 2-5*\n",
    "![Extending our pattern for a more complex example](fig2.5.png)\n",
    "\n",
    "```python\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "```\n",
    "\n",
    "The first, `Flatten` isn't a layer of neurons, but an input layer specification. Our inputs are $28\\times 28$ images, but we want them to be treated as a series of numeric values, like a gray boxes at the top of the *Figure 2-5*. `Flatten` takes that \"square\" value (a 2D array) and turns it into a line (a 1D array).\n",
    "\n",
    "The next one, `Dense`, is a layer of neurons, and we're specifying that we want 128 of them. This is the middle layer shown in *Figure 2-5*. You'll often hear such layers described as $\\textbf{\\textit{hidden layers}}$. Layers that are between the inputs and the outputs aren't seen by a caller, so the term \"hidden\" is used to describe them. We are asking for $128$ neurons to have their internal parameters randomly initialized. More neurons means it will run more slowly, as it has to learn more parameters. More neurons could also lead to a network that is greate at recognizing the training data, but not so good at recognizing data that it hasn't previously seen (this is known as $\\textit{overfitting}$). On the other hand, fewer neurons means that the model might not have sufficient parameters to learn.\n",
    "\n",
    "It takes some experimentation over time to pick the right values. This process is typically called $\\textbf{\\textit{hyperparameter\n",
    "tuning}}$. In machine learning, a hyperparameter is a value that is used to control the training, as opposed to the internal values of the neurons that get trained/learned, which are referred to as parameters.\n",
    "\n",
    "There's also an $\\textit{activation function}$ specified in that layer. The activation function is code that will execute on each neuron in the layer. TensorFlow supports a number of them, but a very common one in middle layers is `relu`, which stands for $\\textit{rectified linear unit}$. It's a simple function that just returns a value if it's greater than $0$. In this case, we don't want negative values being passed to the next layer to potentially impact the summing function, so instead of writing a lot of `if-then` code, we can simply activate the layer with `relu`.\n",
    "\n",
    "Finally, there's another `Dense` layer, which is the output layer. This has $10$ neurons, because we have $10$ classes. Each of these neurons will end up with a probability that the input pixels match that class, so our job is to determine which one has the highest values. We could loop through them to pick that value, but the `softmax` activation function does that for us.\n",
    "\n",
    "Now when we train our neural network, the goal is that we can feed in a $28 \\times 28$-pixel array and the neurons in the middle layer will have weights and bias ($m$ and $c$ values) that when combined will match those pixels to one of the $10$ output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7806 - loss: 0.6321\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8606 - loss: 0.3865\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8741 - loss: 0.3462\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.3136\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8899 - loss: 0.2991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x203c2a16570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First is a handy shortcut for accessing the data:\n",
    "```python\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "```\n",
    "`keras` has a number of built-in datasets that you can access with a single line of code like this. In this case you don't have to handle downloading the $70,000$ images-spliting them into training and test sets, and so on-all it takes is one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call its `load_data` method to return our training and test sets like this:\n",
    "```python\n",
    "(training_images, training_labels), \n",
    "(test_images, test_labels) = data.load_data()\n",
    "```\n",
    "Fashion MNIST is designed to have $60,000$ training images and $10,000$ test images. So the return from `data.load_data` will give you an array of $60,000 $ $28 \\times 28$-pixel arrays called `training_images`, and an array of $60,000$ values (0-9) called `training_labels`. Similarly, the `test_images` will contain $10,000$ 28 $\\times$ 28-pixel arrays, and the `test_labels` array will contain $10,000$ values between 0 and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "```\n",
    "Python allows you to do an operation across the entire array with this notation. Recall that all of the pixels in our images are grayscale, with values between 0 and 255. Dividing by 255 thus ensures that every pixel is represented by a number between 0 and 1 instead. This process is called $\\textbf{\\textit{normalizing}}$ the image.\n",
    "\n",
    "Normalization will improve performance. Often your network will not learn and will have massive errors when dealing with non normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the neural network that makes up our model, as discussed ealier:\n",
    "```python\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "```\n",
    "When we compile our model we specify the loss function and the optimizer as before:\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
