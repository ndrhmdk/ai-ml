{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T11:04:10.164690Z",
     "start_time": "2024-11-18T11:04:03.900901Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neurons for Visions**\n",
    "## **Designing the Neural Network**\n",
    "First, we'll look at the design of the neural network in *Figure 2-5*\n",
    "![Extending our pattern for a more complex example](fig2.5.png)\n",
    "\n",
    "```python\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "```\n",
    "\n",
    "The first, `Flatten` isn't a layer of neurons, but an input layer specification. Our inputs are $28\\times 28$ images, but we want them to be treated as a series of numeric values, like a gray boxes at the top of the *Figure 2-5*. `Flatten` takes that \"square\" value (a 2D array) and turns it into a line (a 1D array).\n",
    "\n",
    "The next one, `Dense`, is a layer of neurons, and we're specifying that we want 128 of them. This is the middle layer shown in *Figure 2-5*. You'll often hear such layers described as $\\textbf{\\textit{hidden layers}}$. Layers that are between the inputs and the outputs aren't seen by a caller, so the term \"hidden\" is used to describe them. We are asking for $128$ neurons to have their internal parameters randomly initialized. More neurons means it will run more slowly, as it has to learn more parameters. More neurons could also lead to a network that is greate at recognizing the training data, but not so good at recognizing data that it hasn't previously seen (this is known as $\\textit{overfitting}$). On the other hand, fewer neurons means that the model might not have sufficient parameters to learn.\n",
    "\n",
    "It takes some experimentation over time to pick the right values. This process is typically called $\\textbf{\\textit{hyperparameter\n",
    "tuning}}$. In machine learning, a hyperparameter is a value that is used to control the training, as opposed to the internal values of the neurons that get trained/learned, which are referred to as parameters.\n",
    "\n",
    "There's also an $\\textit{activation function}$ specified in that layer. The activation function is code that will execute on each neuron in the layer. TensorFlow supports a number of them, but a very common one in middle layers is `relu`, which stands for $\\textit{rectified linear unit}$. It's a simple function that just returns a value if it's greater than $0$. In this case, we don't want negative values being passed to the next layer to potentially impact the summing function, so instead of writing a lot of `if-then` code, we can simply activate the layer with `relu`.\n",
    "\n",
    "Finally, there's another `Dense` layer, which is the output layer. This has $10$ neurons, because we have $10$ classes. Each of these neurons will end up with a probability that the input pixels match that class, so our job is to determine which one has the highest values. We could loop through them to pick that value, but the `softmax` activation function does that for us.\n",
    "\n",
    "Now when we train our neural network, the goal is that we can feed in a $28 \\times 28$-pixel array and the neurons in the middle layer will have weights and bias ($m$ and $c$ values) that when combined will match those pixels to one of the $10$ output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7806 - loss: 0.6321\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8606 - loss: 0.3865\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8741 - loss: 0.3462\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.3136\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8899 - loss: 0.2991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x203c2a16570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First is a handy shortcut for accessing the data:\n",
    "```python\n",
    "    data = tf.keras.datasets.fashion_mnist\n",
    "```\n",
    "`keras` has a number of built-in datasets that you can access with a single line of code like this. In this case you don't have to handle downloading the $70,000$ images-spliting them into training and test sets, and so on-all it takes is one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call its `load_data` method to return our training and test sets like this:\n",
    "```python\n",
    "    (training_images, training_labels), \n",
    "    (test_images, test_labels) = data.load_data()\n",
    "```\n",
    "Fashion MNIST is designed to have $60,000$ training images and $10,000$ test images. So the return from `data.load_data` will give you an array of $60,000 $ $28 \\times 28$-pixel arrays called `training_images`, and an array of $60,000$ values (0-9) called `training_labels`. Similarly, the `test_images` will contain $10,000$ 28 $\\times$ 28-pixel arrays, and the `test_labels` array will contain $10,000$ values between 0 and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    training_images  = training_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "```\n",
    "Python allows you to do an operation across the entire array with this notation. Recall that all of the pixels in our images are grayscale, with values between 0 and 255. Dividing by 255 thus ensures that every pixel is represented by a number between 0 and 1 instead. This process is called $\\textbf{\\textit{normalizing}}$ the image.\n",
    "\n",
    "Normalization will improve performance. Often your network will not learn and will have massive errors when dealing with non normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the neural network that makes up our model, as discussed ealier:\n",
    "```python\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "```\n",
    "When we compile our model we specify the loss function and the optimizer as before:\n",
    "```python\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "```\n",
    "The loss function in this case is called $\\textit{sparse categorical cross entropy}$, and it's one of the arsenal of loss functions that are built into TensorFlow. Again, choosing which loss function to use is an art in itself, and over time you'll learn which ones are best to use in which scenarios. Here we're picking a *category*. Our item of clothing will belong to $1$ of $10$ categories of clothing, and thus using a *categorical* loss function is the way to go. Sparse categorical cross entropy is a good choice\n",
    "\n",
    "The same applies to choosing an optimizer. The `adam` optimizer is an evolution of the stochastic gradient descent (`sgd`) optimizers that has been shown to be faster and more efficient. As we're handling $60,000$ training images, any performance improvement we can get will be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train the network by fitting the training images to the training labels over five epochs:\n",
    "```python\n",
    "    model.fit(training_images, training_labels, epochs=5)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do something new-evaluate the model, using a single line of code. We have a set of $10,000$ images and lebels for testing, and we can pass them to the trained model to have it predict what it thinks each image is, compare that to its actual label, and sum up the result:\n",
    "```python\n",
    "    model.evaluate(test_images, test_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the Neural Network**\n",
    "```python\n",
    "    58016/60000 [=====>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8907\n",
    "    59552/60000 [=====>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8906\n",
    "    60000/60000 [] - 2s 34us/sample - loss: 0.2940 - accuracy: 0.8906\n",
    "```\n",
    "Note that now it's now reporting accuracy. So in this case, using the training data, our model ended up with an accuracy of about $89\\%$ after only five epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the test data? The result of `model.evaluate` on our test data will look something like this:\n",
    "```python\n",
    "    10000/1 [====] - 0s 30us/sample - loss: 0.2521 - accuracy: 0.8736\n",
    "```\n",
    "In this case the accuracy of the model was $87.36\\%$, which isn't bad considering we only trained it for five epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably wondering why the accuracy is *lower* for the test data than it is for the training data. This is very commonly seen but it makes sense: the neural network only really knows how to match the inputs it has been trained on with the ouputs for those values. Our hope is that, given enough data, it will be able to generalize from the examples it has seen, \"learning\" what a shoe or a dress looks like. But there will always be examples of items that is hasn't seen that are sufficiently different from what it has to confuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring the Model Output**\n",
    "Now that the model has been trained, and we have a good gage of its accuracy using the test set, let's explore it a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step\n",
      "[5.6368339e-07 2.8035640e-07 6.5099579e-07 5.6527142e-07 1.2278863e-06\n",
      " 9.3436383e-02 1.2970603e-05 3.5550106e-02 1.4503153e-04 8.7085217e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification gives us back an array of values. These are the values of the $10& output neurons. The label is the actual label for the item of clothing, in this case $9$. You'll see that some of the values are very small, and the last one is the largest by far. These are the probabilities that the image matches the label at that particular index. So, what the neural network is reporting is that there's a $91.4\\%$ chance that the item of clothing at index $0$ is label $9$. We know that it's label $9$, so it got it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training for Longer-Discovering Overfitting**\n",
    "In this case, we trained for only five epochs. That is, we went through the entire training loop of having the neurons randomly initialized, checked against their labels, having that performance measured by the loss function, and then updated by the optimizer five times. And the result we got were pretty good: $89\\%$ accuracy on the training set and $87\\%$ on the test set.\n",
    "\n",
    "Try to update it to train for $50$ epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
