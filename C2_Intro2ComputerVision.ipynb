{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T11:04:10.164690Z",
     "start_time": "2024-11-18T11:04:03.900901Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neurons for Visions**\n",
    "## **Designing the Neural Network**\n",
    "First, we'll look at the design of the neural network in *Figure 2-5*\n",
    "![Extending our pattern for a more complex example](fig2.5.png)\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "```\n",
    "\n",
    "The first, `Flatten` isn't a layer of neurons, but an input layer specification. Our inputs are $28\\times 28$ images, but we want them to be treated as a series of numeric values, like a gray boxes at the top of the *Figure 2-5*. `Flatten` takes that \"square\" value (a 2D array) and turns it into a line (a 1D array).\n",
    "\n",
    "The next one, `Dense`, is a layer of neurons, and we're specifying that we want 128 of them. This is the middle layer shown in *Figure 2-5*. You'll often hear such layers described as $\\textbf{\\textit{hidden layers}}$. Layers that are between the inputs and the outputs aren't seen by a caller, so the term \"hidden\" is used to describe them. We are asking for $128$ neurons to have their internal parameters randomly initialized. More neurons means it will run more slowly, as it has to learn more parameters. More neurons could also lead to a network that is greate at recognizing the training data, but not so good at recognizing data that it hasn't previously seen (this is known as $\\textit{overfitting}$). On the other hand, fewer neurons means that the model might not have sufficient parameters to learn.\n",
    "\n",
    "It takes some experimentation over time to pick the right values. This process is typically called $\\textbf{\\textit{hyperparameter\n",
    "tuning}}$. In machine learning, a hyperparameter is a value that is used to control the training, as opposed to the internal values of the neurons that get trained/learned, which are referred to as parameters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
